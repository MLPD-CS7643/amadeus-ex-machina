{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# System imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'notebooks' to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # Move one level up\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from solver import Solver\n",
    "from griddy.griddy_tuna import hit_griddy, SearchMethod\n",
    "from models.CRNN import CRNNModel\n",
    "from data.data_loader import MirDataProcessor, ChordDataProcessor\n",
    "from utils.model_utils import get_device\n",
    "from solver import TrialMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f\"Device is {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have already ran the downloader, change the value of download to False\n",
    "download = False\n",
    "\n",
    "# Reprocess for different dataset type while bypassing download\n",
    "reprocess = False\n",
    "\n",
    "# Download and build useable train/test data out of the MIR Billboard dataset\n",
    "data_processer = MirDataProcessor(output_dir='data', download=download, batch_size=256, process_sequential=True, seq_length=8, overlap_sequence=True, use_median=True) # your notebook should be in its own directory to begin with, this should create the \"data\" folder inside that\n",
    "if download:\n",
    "    data_processer.process_billboard_data(log_fail_only=False) # you may need to reprocess the downloaded data into sequential or tabular based on your model\n",
    "if reprocess:\n",
    "    data_processer.dataset.download(partial_download=['metadata'])\n",
    "    data_processer.process_billboard_data(combined_notation=True, chord_vocab='majmin7inv', log_fail_only=False)\n",
    "    # combined notation is standard billboard notation (C:maj), setting False creates separate CSVs for root and chord_class\n",
    "\n",
    "# dataset options: 'combined', 'root', 'chord_class'\n",
    "#train_loader, test_loader, num_classes = data_processer.build_data_loaders(device=device, dataset='combined', nrows=None)\n",
    "train_loader, test_loader, num_classes = data_processer.build_data_loaders(device=device, dataset='root', nrows=None) # set nrows to shrink dataset for testing\n",
    "#train_loader, test_loader, num_classes = data_processer.build_data_loaders(device=device, dataset='chord_class', nrows=2000) # set nrows to shrink dataset for testing\n",
    "\n",
    "#print(f\"Number of root classes: {root_num_classes}\")\n",
    "print(f\"Number of chord classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_processor_spectro = ChordDataProcessor(\n",
    "    device=device,\n",
    "    #process_sequential=True\n",
    ")\n",
    "\n",
    "chord_train_loader, chord_test_loader, chord_num_classes = training_data_processor_spectro.process_all_and_build_loaders(\n",
    "    chord_json_path=\"chord_ref.json\", \n",
    "    notation=\"chord_class\", \n",
    "    mode=\"spectrogram\", \n",
    "    jsontype=\"keyed\", #\"keyed\"\n",
    "    audio_path=\"wav\", #\"timbral_bias_datasets/train/processed\",\n",
    "    batch_size=256,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "data_directory = 'data'\n",
    "# Save the training data loader\n",
    "with open(f\"{data_directory}{os.path.sep}chord_train_loader.pkl\", \"wb\") as train_file:\n",
    "    pickle.dump(chord_train_loader, train_file)\n",
    "\n",
    "# Save the testing data loader\n",
    "with open(f\"{data_directory}{os.path.sep}chord_test_loader.pkl\", \"wb\") as test_file:\n",
    "    pickle.dump(chord_test_loader, test_file)\n",
    "\n",
    "# Save the number of classes\n",
    "with open(f\"{data_directory}{os.path.sep}chord_num_classes.pkl\", \"wb\") as classes_file:\n",
    "    pickle.dump(chord_num_classes, classes_file)\n",
    "\n",
    "print(\"Data loaders and number of classes saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_processor = ChordDataProcessor(\n",
    "    device=device,\n",
    "    #process_sequential=True\n",
    ")\n",
    "\n",
    "chord_train_loader, chord_test_loader, chord_num_classes = training_data_processor.process_all_and_build_loaders(\n",
    "    chord_json_path=\"chord_ref.json\", \n",
    "    notation=\"chord_class\", \n",
    "    mode=\"chroma\", \n",
    "    jsontype=\"keyed\", #\"keyed\"\n",
    "    audio_path=\"wav\", #\"timbral_bias_datasets/train/processed\",\n",
    "    batch_size=256,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "data_directory = 'data'\n",
    "# Save the training data loader\n",
    "with open(f\"{data_directory}{os.path.sep}chord_train_loader.pkl\", \"wb\") as train_file:\n",
    "    pickle.dump(chord_train_loader, train_file)\n",
    "\n",
    "# Save the testing data loader\n",
    "with open(f\"{data_directory}{os.path.sep}chord_test_loader.pkl\", \"wb\") as test_file:\n",
    "    pickle.dump(chord_test_loader, test_file)\n",
    "\n",
    "# Save the number of classes\n",
    "with open(f\"{data_directory}{os.path.sep}chord_num_classes.pkl\", \"wb\") as classes_file:\n",
    "    pickle.dump(chord_num_classes, classes_file)\n",
    "\n",
    "print(\"Data loaders and number of classes saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data loader\n",
    "with open(f\"{data_directory}{os.path.sep}chord_train_loader.pkl\", \"rb\") as train_file:\n",
    "    chord_train_loader = pickle.load(train_file)\n",
    "\n",
    "# Load the testing data loader\n",
    "with open(f\"{data_directory}{os.path.sep}chord_test_loader.pkl\", \"rb\") as test_file:\n",
    "    chord_test_loader = pickle.load(test_file)\n",
    "\n",
    "# Load the number of classes\n",
    "with open(f\"{data_directory}{os.path.sep}chord_num_classes.pkl\", \"rb\") as classes_file:\n",
    "    chord_num_classes = pickle.load(classes_file)\n",
    "\n",
    "print(\"Data loaders and number of classes loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Do not assume these values are anything but trash, they're just here for testing\n",
    "\n",
    "SOLVER_PARAMS = {\n",
    "    Solver : {\n",
    "        \"device\": device,\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 20,\n",
    "        \"early_stop_epochs\": 5, # early stop after n epochs without improvement, 0 to disable\n",
    "        \"warmup_epochs\": 0, # 0 to disable\n",
    "        \"dtype\": \"float16\",\n",
    "        \"train_dataloader\": chord_train_loader, # must be DataLoader object\n",
    "        \"valid_dataloader\": chord_test_loader, # must be DataLoader object\n",
    "    }\n",
    "}\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    CRNNModel: {\n",
    "        \"input_features\": 12,\n",
    "        \"num_classes\": chord_num_classes,\n",
    "        \"hidden_size\": 512,\n",
    "        \"cnn_params\": {\n",
    "            \"n_blocks\": 1,\n",
    "            \"block_depth\": 3,\n",
    "            \"pad\": 1,\n",
    "            \"stride\": 1,\n",
    "            \"k_conv\": 3,\n",
    "            \"dropout\": 0.2,\n",
    "            \"out_channels\": 64\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "OPTIM_PARAMS = {\n",
    "    torch.optim.Adam : {\n",
    "        \"lr\": 0.001,\n",
    "    }\n",
    "}\n",
    "\n",
    "SCHED_PARAMS = {\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau : {\n",
    "        \"patience\": 3,\n",
    "        \"verbose\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "CRITERION_PARAMS = {\n",
    "    torch.nn.CrossEntropyLoss : {}\n",
    "}\n",
    "\n",
    "PARAM_SET = {\n",
    "    \"solver\": SOLVER_PARAMS,\n",
    "    \"model\" : MODEL_PARAMS,\n",
    "    \"optim\" : OPTIM_PARAMS,\n",
    "    \"sched\" : SCHED_PARAMS,\n",
    "    \"criterion\" : CRITERION_PARAMS,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = list(MODEL_PARAMS.keys())[0](**MODEL_PARAMS[list(MODEL_PARAMS.keys())[0]])\n",
    "optimizer = list(OPTIM_PARAMS.keys())[0](**(OPTIM_PARAMS[list(OPTIM_PARAMS.keys())[0]] | {'params': model.parameters()}))\n",
    "scheduler = list(SCHED_PARAMS.keys())[0](**(SCHED_PARAMS[list(SCHED_PARAMS.keys())[0]] | {'optimizer': optimizer}))\n",
    "criterion = list(CRITERION_PARAMS.keys())[0](**CRITERION_PARAMS[list(CRITERION_PARAMS.keys())[0]])\n",
    "solver = Solver(**(SOLVER_PARAMS[Solver] | {'model': model, 'optimizer': optimizer, 'scheduler': scheduler, 'criterion': criterion}))\n",
    "\n",
    "solver.train_and_evaluate(plot_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def save_history(solver, filename='history.pkl'):\n",
    "    # Create a DataFrame from the history lists\n",
    "    history_df = pd.DataFrame({\n",
    "        'Train Accuracy': solver.train_accuracy_history,\n",
    "        'Validation Accuracy': solver.valid_accuracy_history,\n",
    "        'Train Loss': solver.train_loss_history,\n",
    "        'Validation Loss': solver.valid_loss_history\n",
    "    })\n",
    "\n",
    "    # Pickle the DataFrame to the specified file\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(history_df, file)\n",
    "\n",
    "    print(f'History saved to {filename}')\n",
    "\n",
    "def load_history(self, filename='history.pkl'):\n",
    "    # Load the pickled DataFrame from the file\n",
    "    with open(filename, 'rb') as file:\n",
    "        history_df = pickle.load(file)\n",
    "\n",
    "    # If the class attributes need to be repopulated from the DataFrame:\n",
    "    self.train_accuracy_history = history_df['Train Accuracy'].tolist()\n",
    "    self.valid_accuracy_history = history_df['Validation Accuracy'].tolist()\n",
    "    self.train_loss_history = history_df['Train Loss'].tolist()\n",
    "    self.valid_loss_history = history_df['Validation Loss'].tolist()\n",
    "\n",
    "    print(f'History loaded from {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_history(solver, 'CRNN_history_class.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_study = \"crnn\"\n",
    "\n",
    "output_folder = Path(\"griddy\")\n",
    "\n",
    "hit_griddy(my_study, param_set=PARAM_SET, out_dir=output_folder, n_trials=2, n_jobs=2, prune=False, resume=False, trial_metric=TrialMetric.LOSS)\n",
    "# NOTE: modest values of n_trials and n_jobs set here for testing, set your values accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amadeus-ex-machina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
