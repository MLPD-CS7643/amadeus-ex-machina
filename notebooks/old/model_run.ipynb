{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update conda environment\n",
    "# !conda env create -f ../environment.yaml\n",
    "# !conda env update -f ../environment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate conda environment\n",
    "!conda activate amadeus-ex-machina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'notebooks' to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move one level up\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Test if the path is correctly added\n",
    "print(f\"Parent directory added to sys.path: {parent_dir}\")\n",
    "\n",
    "# Import packages\n",
    "import json\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import models\n",
    "from models.CRNN import CRNNModel\n",
    "from models.CNN import CNNModel\n",
    "from models.RNN import RNNModel\n",
    "from models.AudioDataset import ChordDataset\n",
    "\n",
    "json_file = \"../datagen/chords/chord_ref.json\"\n",
    "audio_dir = \"../datagen/chords/midi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Load the JSON metadata\n",
    "with open(json_file, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Create a mapping for chord classes to integers\n",
    "chord_classes = sorted({value[\"chord_class\"] for value in metadata.values()})\n",
    "chord_class_to_idx = {chord: idx for idx, chord in enumerate(chord_classes)}\n",
    "print(\"Chord Class to Index Mapping:\", chord_class_to_idx)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = ChordDataset(metadata, audio_dir, chord_class_to_idx)\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Move model to the selected device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get the number of classes\n",
    "num_classes = len(chord_class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modular training and validation function\n",
    "\n",
    "def training_validation(model):\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Loop over the batches in the training dataset\n",
    "        for spectrograms, labels in train_loader:\n",
    "            # Move data to the selected device\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "            # Zero gradients from the previous step\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(spectrograms)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # Print training loss for the current epoch\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Evaluate on validation data after every epoch\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "            for spectrograms, labels in val_loader:\n",
    "                spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "                outputs = model(spectrograms)\n",
    "                _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100 * correct / total\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CRNN model\n",
    "crnn_model = CRNNModel(input_channels=2, num_classes=num_classes, hidden_size=128).to(device)\n",
    "training_validation(crnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN model\n",
    "cnn_model = CNNModel(input_channels=2, num_classes=num_classes).to(device)\n",
    "training_validation(cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the flattened size of the spectrograms\n",
    "for spectrograms, _ in train_loader:\n",
    "    rnn_input_size = spectrograms.view(spectrograms.size(0), -1).size(1)\n",
    "    break\n",
    "\n",
    "# Initialize the RNN model\n",
    "rnn_model = RNNModel(input_size=rnn_input_size, hidden_size=128, output_size=num_classes).to(device)\n",
    "training_validation(rnn_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amadeus-ex-machina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
