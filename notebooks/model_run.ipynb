{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or update conda environment\n",
    "# !conda env create -f ../environment.yaml\n",
    "!conda env update -f ../environment.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate conda environment\n",
    "!conda activate amadeus-ex-machina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent directory added to sys.path: /Users/dananabulsi/Desktop/CODING/GitHub-Repos/amadeus-ex-machina\n"
     ]
    }
   ],
   "source": [
    "# System imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'notebooks' to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Move one level up\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "\n",
    "# Test if the path is correctly added\n",
    "print(f\"Parent directory added to sys.path: {parent_dir}\")\n",
    "\n",
    "# Import packages\n",
    "import json\n",
    "import torchaudio\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import models\n",
    "from models.CRNN import CRNNModel\n",
    "from models.CNN import CNNModel\n",
    "from models.RNN import RNNModel\n",
    "from models.AudioDataset import ChordDataset\n",
    "\n",
    "json_file = \"../datagen/chords/chord_ref.json\"\n",
    "audio_dir = \"../datagen/chords/midi/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chord Class to Index Mapping: {'dim': 0, 'maj7': 1, 'sus2': 2}\n",
      "Epoch [1/20], Loss: 1.1368\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [2/20], Loss: 0.7918\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [3/20], Loss: 0.7176\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [4/20], Loss: 0.6965\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [5/20], Loss: 0.6871\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [6/20], Loss: 0.6746\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [7/20], Loss: 0.6641\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [8/20], Loss: 0.6488\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [9/20], Loss: 0.6292\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [10/20], Loss: 0.6067\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [11/20], Loss: 0.6249\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [12/20], Loss: 0.5853\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [13/20], Loss: 0.6069\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [14/20], Loss: 0.6019\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [15/20], Loss: 0.5964\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [16/20], Loss: 0.5673\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [17/20], Loss: 0.5439\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [18/20], Loss: 0.5325\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [19/20], Loss: 0.4957\n",
      "Validation Accuracy: 0.00%\n",
      "Epoch [20/20], Loss: 0.5145\n",
      "Validation Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON metadata\n",
    "with open(json_file, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Create a mapping for chord classes to integers\n",
    "chord_classes = sorted({value[\"chord_class\"] for value in metadata.values()})\n",
    "chord_class_to_idx = {chord: idx for idx, chord in enumerate(chord_classes)}\n",
    "print(\"Chord Class to Index Mapping:\", chord_class_to_idx)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = ChordDataset(metadata, audio_dir, chord_class_to_idx)\n",
    "\n",
    "# Split into training and validation datasets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Move model to the selected device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "num_classes = len(chord_class_to_idx)\n",
    "model = CRNNModel(input_channels=2, num_classes=num_classes, hidden_size=128).to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Loop over the batches in the training dataset\n",
    "    for spectrograms, labels in train_loader:\n",
    "        # Move data to the selected device\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients from the previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(spectrograms)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print training loss for the current epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate on validation data after every epoch\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for spectrograms, labels in val_loader:\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            outputs = model(spectrograms)\n",
    "            _, predicted = torch.max(outputs, 1)  # Get predicted class\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amadeus-ex-machina",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
