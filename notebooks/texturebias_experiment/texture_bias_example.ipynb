{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import sys\n",
    "# Add the parent directory of 'notebooks' to sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"../..\"))  # Move one level up\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "from datagen.chordgen import __generate_midi_chord as generate_midi_chord, __synthesize_to_wav as synthesize_to_wav, __note_lookup as note_lookup, CHORDS, INVERSIONS, GM_INSTRUMENTS, JSON_FILE\n",
    "from datagen.fxgen_torch import TorchFXGenerator\n",
    "from datagen.pedals import Distortion, Chorus, Delay, Reverb, Noise\n",
    "from utils.gdrive import download_from_gdrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset characteristics\n",
    "TRAIN_SIZE = 10 #2000\n",
    "VAL_SIZE = 500\n",
    "TEST_SIZE = 1000\n",
    "BASE_PATH = Path(\"./timbral_bias_datasets\")\n",
    "SF2_SUBDIR = \"sf2\"\n",
    "WAV_SUBDIR = \"wav\"\n",
    "SF2_ARCHIVE = \"FluidR3_GM.sf2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised genre-based chord groupings ensuring all chords are represented\n",
    "# Primary chords for each genre (high probability)\n",
    "METAL_PRIMARY = [\"1\", \"5\", \"sus4\", \"sus4(b7)\", \"min\", \"dim\"]\n",
    "POP_PRIMARY = [\"maj\", \"min\", \"sus2\", \"sus4\", \"maj6\", \"7\", \"maj/2\", \"maj/4\", \"min/2\", \"min/4\"]\n",
    "JAZZ_PRIMARY = [\"maj7\", \"min7\", \"maj9\", \"min9\", \"11\", \"13\", \"maj6(9)\", \"minmaj7\", \"hdim7\", \"dim7\"]\n",
    "\n",
    "# Secondary chords (lower probability but ensures representation)\n",
    "METAL_SECONDARY = list(set(CHORDS.keys()) - set(METAL_PRIMARY))\n",
    "POP_SECONDARY = list(set(CHORDS.keys()) - set(POP_PRIMARY))\n",
    "JAZZ_SECONDARY = list(set(CHORDS.keys()) - set(JAZZ_PRIMARY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_soundfonts(base_path: Path):\n",
    "    \"\"\"\n",
    "    Download and set up soundfonts if not already present.\n",
    "    Returns path to the base directory containing sf2 folder.\n",
    "    \"\"\"\n",
    "    # Move sf2 directory to base level, not within temp\n",
    "    sf2_dir = base_path.parent / SF2_SUBDIR  # Move up one level from dataset dir\n",
    "    sf2_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sf_filepath = sf2_dir / SF2_ARCHIVE\n",
    "    \n",
    "    if not sf_filepath.exists():\n",
    "        print(f\"Downloading soundfont to {sf_filepath}\")\n",
    "        download_from_gdrive(SF2_ARCHIVE, str(sf_filepath.absolute()))\n",
    "    else:\n",
    "        print(\"Soundfont already exists, skipping download\")\n",
    "    \n",
    "    return sf_filepath.parent  # Return the directory containing sf2 folder\n",
    "\n",
    "def select_chords_for_genre(primary_chords, secondary_chords, primary_weight=0.8, count=100):\n",
    "    \"\"\"\n",
    "    Select chords for a genre with weighted probability.\n",
    "    \n",
    "    Args:\n",
    "        primary_chords: List of primary chords for the genre\n",
    "        secondary_chords: List of secondary chords for the genre\n",
    "        primary_weight: Probability weight for primary chords\n",
    "        count: Number of chords to select\n",
    "    \"\"\"\n",
    "    primary_count = int(count * primary_weight)\n",
    "    secondary_count = count - primary_count\n",
    "    \n",
    "    selections = (\n",
    "        random.choices(primary_chords, k=primary_count) +\n",
    "        random.choices(secondary_chords, k=secondary_count)\n",
    "    )\n",
    "    return selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define genre-appropriate instruments\n",
    "METAL_INSTRUMENTS = {\n",
    "    29: \"overdriven_guitar\",\n",
    "    30: \"distortion_guitar\",\n",
    "    27: \"electric_guitar_(clean)\"  # For some variety\n",
    "}\n",
    "\n",
    "POP_INSTRUMENTS = {\n",
    "    80: \"lead_1_(square)\",\n",
    "    81: \"lead_2_(sawtooth)\", \n",
    "    4: \"electric_piano_1\",\n",
    "    5: \"electric_piano_2\",\n",
    "    27: \"electric_guitar_(clean)\"\n",
    "}\n",
    "\n",
    "JAZZ_INSTRUMENTS = {\n",
    "    0: \"acoustic_grand_piano\",\n",
    "    24: \"acoustic_guitar_(nylon)\",\n",
    "    26: \"electric_guitar_(jazz)\",\n",
    "    4: \"electric_piano_1\"\n",
    "}\n",
    "\n",
    "# FX combinations remain the same\n",
    "METAL_FX = {\n",
    "    'distortion': ['classic_distortion', 'fuzz'],\n",
    "    'noise': ['room_noise'],\n",
    "    'reverb': ['small_room']\n",
    "}\n",
    "\n",
    "POP_FX = {\n",
    "    'reverb': ['plate', 'large_hall'],\n",
    "    'chorus': ['subtle', 'classic'],\n",
    "    'distortion': ['subtle_drive']\n",
    "}\n",
    "\n",
    "JAZZ_FX = {\n",
    "    'reverb': ['large_hall', 'plate'],\n",
    "    'chorus': ['subtle'],\n",
    "    'delay': ['subtle']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_mp3(wav_path: Path, target_path: Path, sample_rate: int = 44100, bitrate: float = 192.0):\n",
    "    \"\"\"\n",
    "    Convert WAV file to MP3 using torchaudio.\n",
    "    \n",
    "    Args:\n",
    "        wav_path: Path to source WAV file\n",
    "        target_path: Path to output MP3 file\n",
    "        sample_rate: Target sample rate\n",
    "        bitrate: Target bitrate in kbps\n",
    "    \"\"\"\n",
    "    waveform, sr = torchaudio.load(str(wav_path))\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Ensure correct sample rate\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
    "    \n",
    "    # Save as MP3\n",
    "    torchaudio.save(\n",
    "        str(target_path),\n",
    "        waveform,\n",
    "        sample_rate,\n",
    "        format=\"mp3\",\n",
    "        compression=bitrate/1000  # torchaudio expects compression rate in kbps/1000\n",
    "    )\n",
    "\n",
    "def batch_convert_wavs_to_mp3(wav_dir: Path, mp3_dir: Path):\n",
    "    \"\"\"\n",
    "    Convert all WAV files in a directory to MP3.\n",
    "    Updates metadata to reflect MP3 filenames.\n",
    "    \n",
    "    Args:\n",
    "        wav_dir: Directory containing WAV files\n",
    "        mp3_dir: Directory to save MP3 files\n",
    "    \"\"\"\n",
    "    mp3_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load metadata\n",
    "    json_path = wav_dir.parent / JSON_FILE\n",
    "    with open(json_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Convert each file and update metadata\n",
    "    updated_metadata = {}\n",
    "    for wav_file in wav_dir.glob(\"*.wav\"):\n",
    "        mp3_file = mp3_dir / wav_file.with_suffix('.mp3').name\n",
    "        convert_to_mp3(wav_file, mp3_file)\n",
    "        \n",
    "        # Update metadata\n",
    "        base_name = wav_file.stem\n",
    "        if base_name in metadata:\n",
    "            entry = metadata[base_name].copy()\n",
    "            entry['filename'] = mp3_file.name\n",
    "            entry['format'] = 'mp3'\n",
    "            updated_metadata[base_name] = entry\n",
    "    \n",
    "    # Save updated metadata\n",
    "    with open(mp3_dir.parent / JSON_FILE, 'w') as f:\n",
    "        json.dump(updated_metadata, f)\n",
    "    \n",
    "    return updated_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __save_json(data: dict, path: Path):\n",
    "    \"\"\"Save metadata in the same format as chordgen.\"\"\"\n",
    "    dumps = json.dumps(data)\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    with open(path / JSON_FILE, 'w') as outfile:\n",
    "        outfile.write(dumps)\n",
    "\n",
    "def generate_chord_samples(output_path: Path, chord_list: list, instruments: dict, \n",
    "                         sample_count: int, duration: float = 2.0):\n",
    "    \"\"\"\n",
    "    Generate specific chord samples with selected instruments.\n",
    "    \n",
    "    Args:\n",
    "        output_path: Directory to save generated files\n",
    "        chord_list: List of chord types to generate\n",
    "        instruments: Dictionary of GM preset IDs and names to use\n",
    "        sample_count: Number of samples to generate\n",
    "        duration: Length of audio samples in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of metadata matching chordgen's format\n",
    "    \"\"\"\n",
    "    temp_path = output_path / \"temp\"\n",
    "    wav_dir = temp_path / \"wav\"\n",
    "    sf2_dir = temp_path / \"sf2\"\n",
    "    wav_dir.mkdir(parents=True, exist_ok=True)\n",
    "    sf2_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Calculate samples per chord to meet target count\n",
    "    samples_per_chord = max(1, sample_count // len(chord_list))\n",
    "    \n",
    "    json_out = {}\n",
    "    sf_filepath = sf2_dir / \"FluidR3_GM.sf2\"\n",
    "    \n",
    "    # Generate each chord with selected instruments\n",
    "    for chord_type in chord_list:\n",
    "        # Select random root notes for variety\n",
    "        root_notes = random.choices(range(12), k=samples_per_chord)  # 0-11 for C through B\n",
    "        octaves = random.choices(range(3, 6), k=samples_per_chord)  # Octaves 3-5\n",
    "        \n",
    "        for root, octave in zip(root_notes, octaves):\n",
    "            # Generate MIDI data\n",
    "            midi = generate_midi_chord(root + (octave * 12), CHORDS[chord_type])\n",
    "            note_name = note_lookup(root + (octave * 12))\n",
    "            \n",
    "            # Save MIDI file\n",
    "            mid_filename = f\"{note_name}{chord_type.replace('/','inv')}_O{octave}\"\n",
    "            mid_filepath = wav_dir / f\"{mid_filename}.mid\"\n",
    "            midi.save(mid_filepath)\n",
    "            \n",
    "            # Generate audio for each selected instrument\n",
    "            selected_instruments = random.sample(list(instruments.items()), \n",
    "                                              k=min(2, len(instruments)))  # Select 1-2 instruments\n",
    "            \n",
    "            for preset_id, instrument_name in selected_instruments:\n",
    "                wav_filename = f\"{mid_filename}_{instrument_name}\"\n",
    "                wav_filepath = wav_dir / f\"{wav_filename}.wav\"\n",
    "                \n",
    "                # Synthesize audio\n",
    "                synthesize_to_wav(\n",
    "                    str(mid_filepath.absolute()),\n",
    "                    str(sf_filepath.absolute()),\n",
    "                    str(wav_filepath.absolute()),\n",
    "                    preset_id=preset_id,\n",
    "                    seconds_to_generate=duration,\n",
    "                    gain=-6\n",
    "                )\n",
    "                \n",
    "                # Create metadata entry matching chordgen's format\n",
    "                json_out[wav_filename] = {\n",
    "                    \"root\": note_name,\n",
    "                    \"chord_class\": chord_type,\n",
    "                    \"billboard_notation\": f\"{note_name}:{chord_type}\",\n",
    "                    \"octave\": octave,\n",
    "                    \"instrument\": instrument_name,\n",
    "                    \"gm_preset_id\": preset_id,\n",
    "                    \"filename\": f\"{wav_filename}.wav\",\n",
    "                    \"format\": \"wav\",\n",
    "                    \"duration(s)\": duration,\n",
    "                    \"sample_rate\": 44100,\n",
    "                    \"bit_depth\": 16\n",
    "                }\n",
    "            \n",
    "            # Clean up MIDI file after generating all instrument versions\n",
    "            os.remove(mid_filepath)\n",
    "    \n",
    "    # Save metadata in chord_ref.json\n",
    "    __save_json(json_out, temp_path)\n",
    "    \n",
    "    #return json_out\n",
    "\n",
    "    # After generating all WAVs and metadata\n",
    "    wav_dir = temp_path / \"wav\"\n",
    "    mp3_dir = temp_path / \"mp3\"\n",
    "    \n",
    "    # Convert WAVs to MP3s and update metadata\n",
    "    json_out = batch_convert_wavs_to_mp3(wav_dir, mp3_dir)\n",
    "    \n",
    "    # Clean up WAV files if desired\n",
    "    for wav_file in wav_dir.glob(\"*.wav\"):\n",
    "        wav_file.unlink()\n",
    "    wav_dir.rmdir()\n",
    "    \n",
    "    return json_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio_with_fx_chain(audio_path: Path, output_path: Path, fx_presets: dict):\n",
    "    \"\"\"\n",
    "    Process an audio file with a specific chain of effects.\n",
    "    \n",
    "    Args:\n",
    "        audio_path: Path to input audio file\n",
    "        output_path: Path to save processed audio\n",
    "        fx_presets: Dictionary of effects and their preset lists to apply\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    waveform, sr = torchaudio.load(str(audio_path))\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    audio = waveform.numpy()\n",
    "    \n",
    "    # Apply effects in sequence\n",
    "    processed = audio\n",
    "    applied_fx = {}\n",
    "    \n",
    "    # For each effect type, use the first preset in its list\n",
    "    # This ensures consistent processing within each genre\n",
    "    for fx_type, preset_list in fx_presets.items():\n",
    "        preset_name = preset_list[0]  # Use first preset from list\n",
    "        \n",
    "        if fx_type == 'distortion':\n",
    "            dist = Distortion.Distortion(sr=sr)\n",
    "            preset_params = dist.get_presets()[preset_name]\n",
    "            processed = dist.distort(processed, **preset_params)\n",
    "        elif fx_type == 'reverb':\n",
    "            rev = Reverb.Reverb(sr=sr)\n",
    "            preset_params = rev.get_presets()[preset_name]\n",
    "            processed = rev.reverb(processed, **preset_params)\n",
    "        elif fx_type == 'chorus':\n",
    "            cho = Chorus.Chorus(sr=sr)\n",
    "            preset_params = cho.get_presets()[preset_name]\n",
    "            processed = cho.process(processed, **preset_params)\n",
    "        elif fx_type == 'noise':\n",
    "            noise = Noise.NoiseGenerator(sr=sr)\n",
    "            preset_params = noise.get_presets()[preset_name]\n",
    "            processed = noise.add_noise(processed, **preset_params)\n",
    "            \n",
    "        applied_fx[fx_type] = preset_name\n",
    "    \n",
    "    # Save processed audio\n",
    "    processed_tensor = torch.from_numpy(processed)\n",
    "    torchaudio.save(\n",
    "        str(output_path),\n",
    "        processed_tensor.unsqueeze(0),\n",
    "        sr,\n",
    "        format=\"mp3\",\n",
    "        compression=192/1000\n",
    "    )\n",
    "    \n",
    "    return applied_fx\n",
    "\n",
    "def generate_biased_dataset(output_path: Path, size: int, genre_weights=(0.33, 0.33, 0.34)):\n",
    "    \"\"\"\n",
    "    Generate a dataset with intentional genre-based timbral bias.\n",
    "    \"\"\"\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    processed_dir = output_path / \"processed\"\n",
    "    processed_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Calculate genre-specific sample counts\n",
    "    metal_count = int(size * genre_weights[0])\n",
    "    pop_count = int(size * genre_weights[1])\n",
    "    jazz_count = size - metal_count - pop_count\n",
    "    \n",
    "    all_samples = []\n",
    "    \n",
    "    # Generate samples for each genre using predefined FX configurations\n",
    "    for genre_idx, (primary_chords, secondary_chords, instruments, count, fx_chain) in enumerate([\n",
    "        (METAL_PRIMARY, METAL_SECONDARY, METAL_INSTRUMENTS, metal_count, METAL_FX),\n",
    "        (POP_PRIMARY, POP_SECONDARY, POP_INSTRUMENTS, pop_count, POP_FX),\n",
    "        (JAZZ_PRIMARY, JAZZ_SECONDARY, JAZZ_INSTRUMENTS, jazz_count, JAZZ_FX)\n",
    "    ]):\n",
    "        print(f\"Processing {['metal', 'pop', 'jazz'][genre_idx]} samples...\")\n",
    "        \n",
    "        # Select chords for this genre\n",
    "        selected_chords = select_chords_for_genre(primary_chords, secondary_chords, count=count)\n",
    "        \n",
    "        # Generate samples\n",
    "        genre_metadata = generate_chord_samples(\n",
    "            output_path=output_path,\n",
    "            chord_list=selected_chords,\n",
    "            instruments=instruments,\n",
    "            sample_count=count\n",
    "        )\n",
    "        \n",
    "        # Process each sample with the genre-specific FX chain\n",
    "        for filename, metadata in genre_metadata.items():\n",
    "            input_path = output_path / \"temp\" / \"mp3\" / f\"{filename}.mp3\"\n",
    "            output_path = processed_dir / f\"proc_{filename}.mp3\"\n",
    "            \n",
    "            # Apply FX chain\n",
    "            applied_fx = process_audio_with_fx_chain(input_path, output_path, fx_chain)\n",
    "            \n",
    "            # Update metadata\n",
    "            metadata.update({\n",
    "                'genre': ['metal', 'pop', 'jazz'][genre_idx],\n",
    "                'applied_fx': applied_fx,\n",
    "                'processed_path': str(output_path)\n",
    "            })\n",
    "            all_samples.append(metadata)\n",
    "    \n",
    "    # Save complete dataset metadata\n",
    "    with open(output_path / \"dataset_metadata.json\", 'w') as f:\n",
    "        json.dump(all_samples, f, indent=2)\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    temp_path = output_path / \"temp\"\n",
    "    if temp_path.exists():\n",
    "        for file in temp_path.glob(\"*\"):\n",
    "            if file.is_file():\n",
    "                os.remove(file)\n",
    "        temp_path.rmdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing metal samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emortime/Documents/Masters/DL 7643/MLPD/amadeus-ex-machina/datagen/pedals/Noise.py:45: RuntimeWarning: invalid value encountered in divide\n",
      "  return brown / np.std(brown)\n",
      "Processing chords:   0%|          | 0/3036 [2:19:22<?, ?it/s]\n",
      "Processing chords:   0%|          | 0/3036 [2:08:03<?, ?it/s]\n",
      "Processing chords:   0%|          | 0/3036 [2:06:36<?, ?it/s]\n",
      "Processing chords:   0%|          | 0/3036 [2:03:07<?, ?it/s]\n",
      "Processing chords:   0%|          | 0/3036 [1:42:32<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The length of the input vector x must be greater than padlen, which is 15.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Training set with genre bias\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m generate_biased_dataset(\n\u001b[1;32m      3\u001b[0m     BASE_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     TRAIN_SIZE,\n\u001b[1;32m      5\u001b[0m     genre_weights\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.33\u001b[39m, \u001b[38;5;241m0.34\u001b[39m, \u001b[38;5;241m0.33\u001b[39m)\n\u001b[1;32m      6\u001b[0m )\n",
      "Cell \u001b[0;32mIn[78], line 96\u001b[0m, in \u001b[0;36mgenerate_biased_dataset\u001b[0;34m(output_path, size, genre_weights)\u001b[0m\n\u001b[1;32m     93\u001b[0m output_path \u001b[38;5;241m=\u001b[39m processed_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproc_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# Apply FX chain\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m applied_fx \u001b[38;5;241m=\u001b[39m process_audio_with_fx_chain(input_path, output_path, fx_chain)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Update metadata\u001b[39;00m\n\u001b[1;32m     99\u001b[0m metadata\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetal\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjazz\u001b[39m\u001b[38;5;124m'\u001b[39m][genre_idx],\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplied_fx\u001b[39m\u001b[38;5;124m'\u001b[39m: applied_fx,\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_path\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(output_path)\n\u001b[1;32m    103\u001b[0m })\n",
      "Cell \u001b[0;32mIn[78], line 40\u001b[0m, in \u001b[0;36mprocess_audio_with_fx_chain\u001b[0;34m(audio_path, output_path, fx_presets)\u001b[0m\n\u001b[1;32m     38\u001b[0m         noise \u001b[38;5;241m=\u001b[39m Noise\u001b[38;5;241m.\u001b[39mNoiseGenerator(sr\u001b[38;5;241m=\u001b[39msr)\n\u001b[1;32m     39\u001b[0m         preset_params \u001b[38;5;241m=\u001b[39m noise\u001b[38;5;241m.\u001b[39mget_presets()[preset_name]\n\u001b[0;32m---> 40\u001b[0m         processed \u001b[38;5;241m=\u001b[39m noise\u001b[38;5;241m.\u001b[39madd_noise(processed, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreset_params)\n\u001b[1;32m     42\u001b[0m     applied_fx[fx_type] \u001b[38;5;241m=\u001b[39m preset_name\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Save processed audio\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Masters/DL 7643/MLPD/amadeus-ex-machina/datagen/pedals/Noise.py:81\u001b[0m, in \u001b[0;36mNoiseGenerator.add_noise\u001b[0;34m(self, audio, noise_type, noise_level_db, highpass_freq, lowpass_freq, noise_gate_db)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m highpass_freq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     80\u001b[0m     b, a \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mbutter(\u001b[38;5;241m4\u001b[39m, highpass_freq\u001b[38;5;241m/\u001b[39mnyquist, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhigh\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m     noise \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mfiltfilt(b, a, noise)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lowpass_freq \u001b[38;5;241m<\u001b[39m nyquist:\n\u001b[1;32m     84\u001b[0m     b, a \u001b[38;5;241m=\u001b[39m signal\u001b[38;5;241m.\u001b[39mbutter(\u001b[38;5;241m4\u001b[39m, lowpass_freq\u001b[38;5;241m/\u001b[39mnyquist, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fx_env/lib/python3.11/site-packages/scipy/signal/_signaltools.py:4198\u001b[0m, in \u001b[0;36mfiltfilt\u001b[0;34m(b, a, x, axis, padtype, padlen, method, irlen)\u001b[0m\n\u001b[1;32m   4195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n\u001b[1;32m   4197\u001b[0m \u001b[38;5;66;03m# method == \"pad\"\u001b[39;00m\n\u001b[0;32m-> 4198\u001b[0m edge, ext \u001b[38;5;241m=\u001b[39m _validate_pad(padtype, padlen, x, axis,\n\u001b[1;32m   4199\u001b[0m                           ntaps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(a), \u001b[38;5;28mlen\u001b[39m(b)))\n\u001b[1;32m   4201\u001b[0m \u001b[38;5;66;03m# Get the steady state of the filter's step response.\u001b[39;00m\n\u001b[1;32m   4202\u001b[0m zi \u001b[38;5;241m=\u001b[39m lfilter_zi(b, a)\n",
      "File \u001b[0;32m~/miniconda3/envs/fx_env/lib/python3.11/site-packages/scipy/signal/_signaltools.py:4248\u001b[0m, in \u001b[0;36m_validate_pad\u001b[0;34m(padtype, padlen, x, axis, ntaps)\u001b[0m\n\u001b[1;32m   4246\u001b[0m \u001b[38;5;66;03m# x's 'axis' dimension must be bigger than edge.\u001b[39;00m\n\u001b[1;32m   4247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[axis] \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m edge:\n\u001b[0;32m-> 4248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe length of the input vector x must be greater \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4249\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthan padlen, which is \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m edge)\n\u001b[1;32m   4251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m edge \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;66;03m# Make an extension of length `edge` at each\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;66;03m# end of the input array.\u001b[39;00m\n\u001b[1;32m   4254\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m padtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meven\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: The length of the input vector x must be greater than padlen, which is 15."
     ]
    }
   ],
   "source": [
    "# Training set with genre bias\n",
    "generate_biased_dataset(\n",
    "    BASE_PATH / \"train\",\n",
    "    TRAIN_SIZE,\n",
    "    genre_weights=(0.33, 0.34, 0.33)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets\n",
    "\n",
    "# Training set with genre bias\n",
    "generate_biased_dataset(\n",
    "    BASE_PATH / \"train\",\n",
    "    TRAIN_SIZE,\n",
    "    genre_weights=(0.4, 0.35, 0.25)  # Slightly more metal to emphasize bias\n",
    ")\n",
    "\n",
    "# Validation set with same bias\n",
    "generate_biased_dataset(\n",
    "    BASE_PATH / \"val\",\n",
    "    VAL_SIZE,\n",
    "    genre_weights=(0.4, 0.35, 0.25)\n",
    ")\n",
    "\n",
    "# Balanced test set\n",
    "generate_balanced_test_set(\n",
    "    BASE_PATH / \"test\",\n",
    "    TEST_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train models on biased dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run models on test set and compare results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fx_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
